{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning_Rule \n",
    "\n",
    "## Solution Method (Learning Rule)\n",
    "The learning rule or solution method is the process by which a model adjusts its parameters to minimize the loss function and improve its performance. Here are some common solution methods used in machine learning:\n",
    "\n",
    "#### 1. **Gradient Descent**\n",
    "\n",
    "- **Overview**: Gradient descent is an optimization algorithm used to minimize the loss function by iteratively moving towards the steepest descent, i.e., the negative gradient of the loss function.\n",
    "- Variants:\n",
    "  - **Batch Gradient Descent**: Computes the gradient of the loss function with respect to the entire dataset.\n",
    "  - **Stochastic Gradient Descent (SGD)**: Computes the gradient for each sample and updates the parameters iteratively.\n",
    "  - **Mini-batch Gradient Descent**: Combines the advantages of batch and stochastic gradient descent by updating parameters iteratively with a small subset of the data.\n",
    "\n",
    "\n",
    "**Example of gradient descent update rule**\n",
    "​```python\n",
    "w = w - learning_rate * gradient\n",
    "​```\n",
    "\n",
    "#### 2. **Newton's Method**\n",
    "\n",
    "- **Overview**: Newton's method uses the second-order derivative (Hessian) of the loss function to find the parameter updates. It converges faster than gradient descent but is computationally expensive for large datasets.\n",
    "- **Update Rule**:\n",
    "\n",
    "$$\n",
    "w= w - (H^{-1}) \\nabla L(w)\n",
    "$$\n",
    "\n",
    "another types such as Conjugate Gradient Method, Quasi-Newton Methods  (Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm, ...), Regularization Techniques (L1 Regularization (Lasso), L2 Regularization (Ridge) )\n",
    "\n",
    "#### 3. **Genetic Algorithms**\n",
    "\n",
    "- **Overview**: These are search heuristics that mimic the process of natural selection to find optimal solutions.\n",
    "- **Process**: Includes selection, crossover, and mutation to evolve the parameters over generations.\n",
    "\n",
    "#### 4. **Bayesian Optimization**\n",
    "\n",
    "- **Overview**: Bayesian optimization utilizes Bayesian statistics to minimize a function by constructing a probabilistic model of the function. This model is then employed to select the most promising parameters for evaluation within the actual function.\n",
    "- **Example: Particle Filter in Bayesian Optimization**: Particle filters, also recognized as Sequential Monte Carlo methods, are utilized for estimating the posterior distribution of state variables within dynamic systems. They employ a collection of particles (samples) to depict the posterior distribution and iteratively update them using Bayesian inference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Intro. of PR](..//Introduction/Introduction_1.ipynb#Table-of-Introduction-in-PR)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
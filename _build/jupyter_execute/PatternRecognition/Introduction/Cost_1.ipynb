{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost_Function\n",
    "Its components are error, loss function and adjustment term. \n",
    "\n",
    "$$\n",
    "\\text{E}\\left\\{l\\left( e \\right)  \\right\\}+\\lambda*Reularization Term\n",
    "$$\n",
    "\n",
    "The optimal parameter is obtained by minimizing the average loss function ùêø considering the defined error ùëí on the parameter in question, along with the adjustment term known as knowledge. This process ensures that the parameter achieves its best fit by balancing between minimizing loss and incorporating relevant adjustments based on acquired knowledge.\n",
    "\n",
    "**Explanation of the Above Equation by Example**\n",
    "What is the meaning of $$\\text{E}\\left\\{l\\left( e \\right)  \\right\\}$$ ? Suppose you want to get married and you have many suggestions from your family and friends, some of which you are considering yourself. However, these suggestions might not perfectly match your criteria. This discrepancy is called the error ùëí . It is important to note that the amount of loss caused by this error is not necessarily equivalent to the error itself.\n",
    "\n",
    "For a better understanding, consider this example: suppose one of your criteria for an ideal spouse is having honey-colored eyes and large eyes. If the proposed person has slightly larger eyes than expected, this discrepancy, or error, is minimal. Therefore, even with the error, the loss can be negligible; in other words, the loss function might be zero even with the error. This scenario describes a relaxed loss function ùëô(ùëí). Depending on the problem and data type, a function can be applied to the error to obtain the desired loss.\n",
    "\n",
    "A noteworthy point is that the effect of different errors on the loss function can be considered. Now, for each matchmaking case, a loss is associated with the person being considered. $$l\\left(e_{1} \\right)$$ is the loss caused by choosing the first option, and \n",
    "\n",
    "$$l\\left(e_{n} \\right)$$ \n",
    "\n",
    "is the loss caused by choosing the last option. Here, the need for a **_cost function_** becomes apparent. In other words, the combination of all losses should be taken into account, and a selection should be made such that the chosen spouse results in a minimum combination of losses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Loss Functions\n",
    "\n",
    "Loss functions are used to evaluate the performance of a model. Two main types are mentioned:\n",
    "\n",
    "### Supervised Loss Functions\n",
    "\n",
    "In this type, the desired outcome is known. For example, we know that this pattern is an apple, the price of this item is $2, or this MRI image belongs to a healthy individual. Two examples of supervised loss functions are as follows:\n",
    "\n",
    "**Classification Loss Functions:**\n",
    "\n",
    "- These functions are used for classification problems, where the goal is to categorize samples into different classes.\n",
    "  An example of these functions is the 0-1 loss function, which assigns a loss of 1 for each misclassification and a loss of 0 for correct classifications.\n",
    "- Another commonly used classification loss function is the cross-entropy loss, which is widely used in training neural networks for multi-class classification problems. It measures the performance of a classification model whose output is a probability value between 0 and 1.\n",
    "\n",
    "**Regression Loss Functions:**\n",
    "These functions are used for regression problems, where the goal is to predict numerical values. \n",
    "\n",
    "- Square loss: An example of these functions is the Mean Squared Error (MSE), which calculates the mean of the squares of the differences between predicted and actual values.\n",
    "- Absolute loss: Another example is the Mean Absolute Error (MAE), which measures the mean of the absolute differences between predicted and actual values.\n",
    "- Huber loss: The Huber loss function is another regression loss that is less sensitive to outliers in data than the MSE. It combines the best properties of MSE and MAE by being quadratic when the error is small and linear when the error is large.\n",
    "\n",
    "## Unsupervised Loss Functions\n",
    "\n",
    "In this type, the desired outcome is not known. For instance, in determining an index from a set of data, only the data is available, and the index is unknown. Similarly, in clustering data, we do not know to which cluster a particular data point belongs. Examples include:\n",
    "\n",
    "**Clustering Loss Functions:**\n",
    "\n",
    "These functions are used in clustering problems where the goal is to group similar data points together. An example is the K-means loss function, which aims to minimize the within-cluster sum of squares, effectively measuring the variance within each cluster.\n",
    "\n",
    "**Dimensionality Reduction Loss Functions:**\n",
    "\n",
    "These functions are used in dimensionality reduction techniques, where the goal is to reduce the number of variables under consideration and can be divided into feature selection and feature extraction. An example is the Principal Component Analysis (PCA) loss function, which minimizes the reconstruction error, aiming to retain as much variance as possible in the reduced dimensions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Intro. of PR](..//Introduction/Introduction_1.ipynb#Table-of-Introduction-in-PR)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}